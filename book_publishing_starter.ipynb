{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "book_publishing_starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarreed0/school_stuff/blob/main/book_publishing_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## I. Parse Text Sources\n",
        "First we'll load our text sources and create our vocabulary lists and encoders. \n",
        "\n",
        "There are ways we could do this in pure python, but using the tensorflow data structures and libraries allow us to keep things super-optimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8RbnIjwHGoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65684109-aa5e-4227-8aeb-d9a023165c3b"
      },
      "source": [
        "# Load file data\n",
        "path_to_tom = tf.keras.utils.get_file('tom.txt', 'http://jar.ylimaf.com/books/tom.txt')\n",
        "path_to_odyssey = tf.keras.utils.get_file('od.txt', 'http://jar.ylimaf.com/books/od.txt')\n",
        "path_to_alice = tf.keras.utils.get_file('alice.txt', 'http://jar.ylimaf.com/books/alice.txt')\n",
        "path_to_gatsby = tf.keras.utils.get_file('greatgatsby.txt', 'http://jar.ylimaf.com/books/greatgatsby.txt')\n",
        "tom = open(path_to_tom, 'rb').read().decode(encoding='utf-8')\n",
        "odyssey = open(path_to_odyssey, 'rb').read().decode(encoding='utf-8')\n",
        "alice = open(path_to_alice, 'rb').read().decode(encoding='utf-8')\n",
        "gatsby = open(path_to_gatsby, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of tom sawyer: {} characters'.format(len(tom)))\n",
        "print('Length of the odyssey: {} characters'.format(len(odyssey)))\n",
        "print('Length of alice in wonderland: {} characters'.format(len(alice)))\n",
        "print('Length of the great gatsby: {} characters'.format(len(gatsby)))\n",
        "\n",
        "text = tom + odyssey + alice + gatsby\n",
        "\n",
        "print('Length of total text: {} characters'.format(len(text)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://jar.ylimaf.com/books/tom.txt\n",
            "409600/405439 [==============================] - 0s 1us/step\n",
            "417792/405439 [==============================] - 0s 1us/step\n",
            "Downloading data from http://jar.ylimaf.com/books/od.txt\n",
            "688128/686172 [==============================] - 0s 1us/step\n",
            "696320/686172 [==============================] - 0s 1us/step\n",
            "Downloading data from http://jar.ylimaf.com/books/alice.txt\n",
            "172032/169675 [==============================] - 0s 2us/step\n",
            "180224/169675 [===============================] - 0s 1us/step\n",
            "Downloading data from http://jar.ylimaf.com/books/greatgatsby.txt\n",
            "286720/280160 [==============================] - 0s 1us/step\n",
            "294912/280160 [===============================] - 0s 1us/step\n",
            "Length of tom sawyer: 392547 characters\n",
            "Length of the odyssey: 678814 characters\n",
            "Length of alice in wonderland: 163180 characters\n",
            "Length of the great gatsby: 270608 characters\n",
            "Length of total text: 1505149 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XRnt0XUHUrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2936fd32-4e9f-4158-8958-db38caccd2e7"
      },
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "THE ADVENTURES OF TOM SAWYER\n",
            "\n",
            "\n",
            "By Mark Twain\n",
            "\n",
            "(Samuel Langhorne Clemens)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            "\n",
            "CHAPTER I. Y-o-u-u Tom-Aunt Polly Decides Upon her Duty—Tom Practices\n",
            "Music—The Challenge—A Private Entrance\n",
            "\n",
            "C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SLd7l0HP1Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f4708c-8468-49e6-9056-c3b186d25bd9"
      },
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this \n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153 unique characters\n",
            "['\\t', '\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', 'Æ', 'æ', 'ç', 'é', 'ê', 'ô', 'ù', 'Μ', 'Ν', 'α', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ἀ', 'ἄ', 'ἐ', 'ἔ', 'ἠ', 'ἦ', 'ἰ', 'ἷ', 'Ἰ', 'ὀ', 'ὄ', 'ὡ', 'ὦ', 'ά', 'έ', 'ή', 'ὶ', 'ί', 'ό', 'ὺ', 'ύ', 'ὼ', 'ώ', 'ῆ', 'ῇ', 'ῖ', 'ῥ', 'ῳ', 'ῶ', '\\u200a', '—', '‘', '’', '“', '”', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtjOxL7wQibb"
      },
      "source": [
        "# Next, we'll encode encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters\n",
        "# and their numeric representations\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52bkemreRw8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ea3ed6-32ef-4942-8b82-c09f91b87fe2"
      },
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([49, 76, 79, 78, 66])>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGUnSHjtD_IJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3855fcf5-6493-49d8-d398-0ea6a8922c2a"
      },
      "source": [
        "chars_from_ids(testids)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rghkpLLLjL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed7adcf5-f5f0-4a6d-c36f-2bfffba52b1a"
      },
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Truth'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXMVqTcSpA0"
      },
      "source": [
        "## II. Construct our training data\n",
        "Next we need to construct our training data by building sentence chunks. Each chunk will consist of a sequence of characters and a corresponding \"next sequence\" of the same length showing what would happen if we move forward in the text. This \"next sequence\" becomes our target variable.\n",
        "\n",
        "For example, if this were our text:\n",
        "\n",
        "> It is a truth universally acknowledged, that a single man in possession\n",
        "of a good fortune, must be in want of a wife.\n",
        "\n",
        "And our sequence length was 10 with a step size of 1, our first chunk would be:\n",
        "\n",
        "* Sequence: `It is a tr`\n",
        "* Next Sequence: `t is a tru`\n",
        "\n",
        "Our second chunk would be:\n",
        "\n",
        "* Sequence: `t is a tru`\n",
        "* Next Word: ` is a trut`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PLJWOg2P_fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dccb2c-6abb-4f83-b575-40bd3cfc3b59"
      },
      "source": [
        "# First, create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1505149,), dtype=int64, numpy=array([ 2, 49, 37, ...,  2,  2,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nBqVY6pFpZs"
      },
      "source": [
        "# Now, convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fr28CJxUBtG"
      },
      "source": [
        "# Finally, let's batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNVukmsUFkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f4948f-27fd-4887-8933-8680239a8b34"
      },
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  \n",
            "THE ADVENTURES OF TOM SAWYER\n",
            "\n",
            "\n",
            "By Mark Twain\n",
            "\n",
            "(Samuel Langhorne Clemens)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            "\n",
            "CHAPTER I. \n",
            "--------\n",
            "Target:  THE ADVENTURES OF TOM SAWYER\n",
            "\n",
            "\n",
            "By Mark Twain\n",
            "\n",
            "(Samuel Langhorne Clemens)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTENTS\n",
            "\n",
            "\n",
            "CHAPTER I. Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDdr6xfZYa0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86052e14-ceae-41c5-e15c-bca8ac534693"
      },
      "source": [
        "# Finally, we'll randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
        "\n",
        "    model = tf.keras.models.Sequentla()\n",
        "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
        "    etc...\n",
        "\n",
        "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
        "\n",
        "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
        "\n",
        "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class TextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class \n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # Our model will have three layers:\n",
        "    \n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the \n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    \n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = TextModel(vocab_size, embedding_dim, rnn_units)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C67kN7YAdfSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9637f52-c58b-48cc-9b31-015126ded380"
      },
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 154) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJGL8gCWdsiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce4667a-dabd-4da9-f605-2df2f9849df6"
      },
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  39424     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  157850    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,135,578\n",
            "Trainable params: 4,135,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later. \n",
        "\n",
        "\n",
        "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "a1d94bf3-6963-414c-eb75-1bd91ef76b6e"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(dataset, epochs=20)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "232/232 [==============================] - 46s 184ms/step - loss: 2.5624\n",
            "Epoch 2/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.8904\n",
            "Epoch 3/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.6109\n",
            "Epoch 4/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.4607\n",
            "Epoch 5/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.3690\n",
            "Epoch 6/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.3030\n",
            "Epoch 7/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.2498\n",
            "Epoch 8/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.2036\n",
            "Epoch 9/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.1599\n",
            "Epoch 10/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.1170\n",
            "Epoch 11/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.0736\n",
            "Epoch 12/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 1.0303\n",
            "Epoch 13/20\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.9864\n",
            "Epoch 14/20\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.9429\n",
            "Epoch 15/20\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.8996\n",
            "Epoch 16/20\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.8586\n",
            "Epoch 17/20\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.8193\n",
            "Epoch 18/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 0.7835\n",
            "Epoch 19/20\n",
            "232/232 [==============================] - 44s 184ms/step - loss: 0.7512\n",
            "Epoch 20/20\n",
            "232/232 [==============================] - 44s 183ms/step - loss: 0.7233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casEwxrXcv4Y"
      },
      "source": [
        "## V. Use the model\n",
        "\n",
        "Now that our model has been trained, we can use it to generate text. As mentioned earlier, to do so we have to keep track of its internal state, or memory, so that we can use previous text predictions to inform later ones.\n",
        "\n",
        "However, with RNN generated text, if we always just pick the character with the highest probability, our model tends to get stuck in a loop. So instead we will create a probability distribution of characters for each step, and then sample from that distribution. We can add some variation to this using a paramter known as [\"temperature\"](https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3lhlyfwcqIN"
      },
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    \n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSUghgUFc6ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0fee96-0f36-43c6-a5dd-24f191e4c043"
      },
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['Chapter 1'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    \n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1,Ventapogus took the world step in my homeway from\n",
            " Yether towards missis Jordand. That Dawn sat down, and Tom was above bene, Tom Walks\n",
            "by night for half an hour lay upon Alice. I all right over\n",
            "the first pite drawn old things than she is a very disgraceful sun, whom get out of\n",
            "the cloisters, and sprinkling his way to the library, so it came out, came all\n",
            "powly16 in the Gatsby’s beint on a thich gleaminess to believe she could be very whole counted was\n",
            "going to be some god can come back to Gatsby’s ground,\n",
            "without a worse’ll be told she tried it, you can think of\n",
            "for him, but when we has not send any on holy unbox _I filled with the best\n",
            "of them, but only whitewashed, he is still made a car come back to my own\n",
            "door and shabby our pirates and young clear wholestly she was\n",
            "when I beautifully after me. When, however, others would have done before he\n",
            "found any one (could find it ever lost in asking off the\n",
            "room. He saw this mortal\n",
            "group of the Termaning, and the women up will be as indeed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Cudf3YoaKr"
      },
      "source": [
        "## VI. Next Steps\n",
        "\n",
        "This is a very simple model with one GRU layer and then an output layer. However, considering how simple it is and the fact that we are predicting outputs character by character, the text it produces is pretty amazing. Though it still has a long way to go before publication.\n",
        "\n",
        "There are many other RNN architectures you could try, such as adding additional hidden dense layers, replacing GRU with one or more LSTM layers, combining GRU and LSTM, etc...\n",
        "\n",
        "You could also experiment with better text cleanup to make sure odd punctuation doesn't appear, or finding longer texts to use. If you combine texts from two authors, what happens? Can you generate a Jane Austen stageplay by combining austen and shakespeare texts?\n",
        "\n",
        "Finally, there are a number of hyperparameters to tweak, such as temperature, epochs, batch size, sequence length, etc..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYkggx4if9R1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784b9e68-667b-4e38-f15a-0d8a988e5806"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npSYBF6Jf9Pd"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuiihZR7f9NH"
      },
      "source": [
        "class next_step_model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51E4nZHpf9K8"
      },
      "source": [
        "model = next_step_model(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItFNOagQf9B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49057c3a-6fe2-42f9-8b20-6e0413bc74ba"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 154) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO3jplGOf43Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98e6dda-d719-4c0a-d550-8284b203faeb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"next_step_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  39424     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  157850    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,135,578\n",
            "Trainable params: 4,135,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFmi8wTNglGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2842b31b-f8c9-4e22-c8b0-3e2187b82fa7"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([129,  21, 137,  38,  14,  30, 112,  81,  25,   4,  51,  84,  97,\n",
              "        85,  39,  52,  16,  43,  98,  58, 115,  68,  30,  69, 145,  28,\n",
              "        22,  96,  57,  95,  58, 118, 117,   6,  29,  44,  87,  54,  43,\n",
              "        70, 102,  79,  56,  95, 131,  14, 115,  19,  65,  82,  83, 133,\n",
              "        33,  42,   9, 153,   7,   9,  22, 132,  47,  79, 125,  97,  24,\n",
              "        91,  14, 135,  95,  42, 125,  95,  64, 137,  44,  76,  37,  90,\n",
              "        84,  60,  14, 109,  63,  16,  67,  53,  17,  77,  97,  44,  41,\n",
              "        32,  39,  75,  87, 101,  61,  61,  69, 115])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVuDVJCSglDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01896457-0896-4f6f-bde9-62982cb153bd"
      },
      "source": [
        "print(\"Input:\\n\", chars_from_ids(input_example_batch[0]))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", chars_from_ids(sampled_indices))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tf.Tensor(\n",
            "[b's' b' ' b'a' b'n' b's' b'w' b'e' b'r' b'e' b'd' b',' b' '\n",
            " b'\\xe2\\x80\\x9c' b'T' b'h' b'e' b' ' b'f' b'a' b'u' b'l' b't' b',' b' '\n",
            " b'f' b'a' b't' b'h' b'e' b'r' b',' b' ' b'i' b's' b' ' b'm' b'i' b'n'\n",
            " b'e' b',' b' ' b'a' b'n' b'd' b' ' b'm' b'i' b'n' b'e' b' ' b'o' b'n'\n",
            " b'l' b'y' b';' b' ' b'I' b' ' b'l' b'e' b'f' b't' b'\\n' b't' b'h' b'e'\n",
            " b' ' b's' b't' b'o' b'r' b'e' b' ' b'r' b'o' b'o' b'm' b' ' b'd' b'o'\n",
            " b'o' b'r' b' ' b'o' b'p' b'e' b'n' b',' b' ' b'a' b'n' b'd' b' ' b't'\n",
            " b'h' b'e' b'y' b' ' b'h' b'a'], shape=(100,), dtype=string)\n",
            "\n",
            "Next Char Predictions:\n",
            " tf.Tensor(\n",
            "[b'\\xe1\\xbd\\xa1' b'4' b'\\xe1\\xbd\\xba' b'I' b'-' b'A' b'\\xcf\\x83' b'w' b'8'\n",
            " b'!' b'V' b'z' b'\\xce\\xb3' b'{' b'J' b'W' b'/' b'N' b'\\xce\\xb4' b'_'\n",
            " b'\\xcf\\x86' b'j' b'A' b'k' b'\\xe1\\xbf\\xb3' b';' b'5' b'\\xce\\xb1' b']'\n",
            " b'\\xce\\x9d' b'_' b'\\xe1\\xbc\\x80' b'\\xcf\\x89' b'$' b'?' b'O' b'\\xc3\\x86'\n",
            " b'Y' b'N' b'l' b'\\xce\\xb8' b'u' b'[' b'\\xce\\x9d' b'\\xe1\\xbd\\xb1' b'-'\n",
            " b'\\xcf\\x86' b'2' b'g' b'x' b'y' b'\\xe1\\xbd\\xb5' b'D' b'M' b\"'\"\n",
            " b'\\xe2\\x80\\xa6' b'%' b\"'\" b'5' b'\\xe1\\xbd\\xb3' b'R' b'u' b'\\xe1\\xbc\\xb7'\n",
            " b'\\xce\\xb3' b'7' b'\\xc3\\xaa' b'-' b'\\xe1\\xbd\\xb7' b'\\xce\\x9d' b'M'\n",
            " b'\\xe1\\xbc\\xb7' b'\\xce\\x9d' b'f' b'\\xe1\\xbd\\xba' b'O' b'r' b'H'\n",
            " b'\\xc3\\xa9' b'z' b'b' b'-' b'\\xcf\\x80' b'e' b'/' b'i' b'X' b'0' b's'\n",
            " b'\\xce\\xb3' b'O' b'L' b'C' b'J' b'q' b'\\xc3\\x86' b'\\xce\\xb7' b'c' b'c'\n",
            " b'k' b'\\xcf\\x86'], shape=(100,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_onYs49CglBu"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZPuOwXtgk_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53181bf-df5d-4733-810a-7fecb5021429"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 154)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         5.0366707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1dspy86gk9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e37dc0-3e0a-4a82-b99f-e0e168891989"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153.95659"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43XZ74Vgk6q"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUiQ8_Oagk4K"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmyLNAa8gkye"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2mNZx8fgzK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a9cd49-a9b1-4c05-bea9-2916c3b457ec"
      },
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "232/232 [==============================] - 45s 182ms/step - loss: 2.5585\n",
            "Epoch 2/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 1.8864\n",
            "Epoch 3/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 1.6061\n",
            "Epoch 4/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 1.4579\n",
            "Epoch 5/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 1.3646\n",
            "Epoch 6/30\n",
            "232/232 [==============================] - 44s 183ms/step - loss: 1.2976\n",
            "Epoch 7/30\n",
            "232/232 [==============================] - 44s 183ms/step - loss: 1.2461\n",
            "Epoch 8/30\n",
            "232/232 [==============================] - 44s 183ms/step - loss: 1.1987\n",
            "Epoch 9/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 1.1552\n",
            "Epoch 10/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 1.1113\n",
            "Epoch 11/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 1.0678\n",
            "Epoch 12/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 1.0239\n",
            "Epoch 13/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.9792\n",
            "Epoch 14/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.9332\n",
            "Epoch 15/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.8910\n",
            "Epoch 16/30\n",
            "232/232 [==============================] - 43s 181ms/step - loss: 0.8493\n",
            "Epoch 17/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.8096\n",
            "Epoch 18/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.7744\n",
            "Epoch 19/30\n",
            "232/232 [==============================] - 43s 181ms/step - loss: 0.7437\n",
            "Epoch 20/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.7163\n",
            "Epoch 21/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.6918\n",
            "Epoch 22/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.6727\n",
            "Epoch 23/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.6540\n",
            "Epoch 24/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.6417\n",
            "Epoch 25/30\n",
            "232/232 [==============================] - 43s 181ms/step - loss: 0.6295\n",
            "Epoch 26/30\n",
            "232/232 [==============================] - 44s 181ms/step - loss: 0.6208\n",
            "Epoch 27/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.6104\n",
            "Epoch 28/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.6057\n",
            "Epoch 29/30\n",
            "232/232 [==============================] - 44s 182ms/step - loss: 0.5992\n",
            "Epoch 30/30\n",
            "232/232 [==============================] - 44s 183ms/step - loss: 0.5977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYhhTX0ygzIO"
      },
      "source": [
        "class step(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "#result = tf.strings.join(result)\n",
        "\n",
        "#print(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL9cb13sgzFn"
      },
      "source": [
        "mod = step(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZspBlDoUgzDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872fa8a1-236f-493c-a643-0e86b474b98c"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = mod.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London.\n",
            "\n",
            "“Then you have been so much will tell you something. Don’t want to foller\n",
            "Premendia Souph Outrolic, and tell the women to tell their\n",
            "corns of deladcholf; for the name sprang of facutes and the maids were\n",
            "at once to start with blackful and other people. He had lost three ships,\n",
            "they went on eaten to carry it over with Sic and Mars and\n",
            "Venus to the seats, knew it is bore, but Ulysses\n",
            "had finished his son Unopse with Paris, and Polybus the shadowsponce of the\n",
            "heavy time in it nearest a year-hay, even no girl had grownuple, and then broad day long\n",
            "tone: “how at your giants, Daisy, dee wheeler Clear will\n",
            "my son though this would be the use of wreath, and other wors are a runsom\n",
            "and cease you.”\n",
            "\n",
            "“I wonder.” Better have obliged that Diana feet\n",
            "I fixed on her face for an Antipattito and said, “Stranger, of course you are\n",
            "come over on my return for lonely for what they can sail to this\n",
            "mind. The rail of mine whether my brothers would have been only fool\n",
            "as you lay in the wood,” continued:\n",
            "\n",
            "“G \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.7974913120269775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eBtgywygzBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735c9e55-e4f5-40e3-9f08-f5cc8bfc49f2"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = mod.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'The world seemed like such a peaceful place until the magic tree was discovered in London.\\n\\nVery merely came home, and finally spread a bath broke.\\n\\n\\xe2\\x80\\x9cHe must are.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cWell why don\\xe2\\x80\\x99t you was in Christ!\\xe2\\x80\\x9d thought Alice. \\xe2\\x80\\x9cOne that\\xe2\\x80\\x99s just\\nto you offend,\\xe2\\x80\\x9d I told them to catch the beam of or frain my\\narrivable mortal and find noble. And the King lay when death\\nshate few honsequest long before, because reading everything to\\nsome central delly. They were now satisfied.\\nEach looked like altogether of perfect purpled questions, and had two\\nprecious pieces of clay with thick bedroom in the matter of dark\\nnorth, with a goats for their ring. They found a show, brother and ashaped more than\\nan only sons servant in the other end of the stairs. Then Pieret a mest\\nof making a despect at the objiceid word to be milked:\\n\\n\\xe2\\x80\\x9c\\xe2\\x80\\x98\\xe2\\x80\\x94 I leave these things about you. You suitour\\nor not.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cI believe, it never be able! I shall not ever know\\xe2\\x80\\x94no one could well rest as boasts in\\nhere.\\xe2\\x80\\x9d\\n\\nFor a minute I\\xe2\\x80\\x99d never a fair wind, she had to take them\\nto do, and their yards dust cried of the alley or Laertes. They thr'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London.\\n\\nWith these words he puzzled out, in another perfectly stilled at her\\nwith the cloak of delight, and to carry the battle\\xe2\\x80\\x94it was cleared the strangers of\\nthe accusations that Ulysses said, \\xe2\\x80\\x9cDo Is I, Tom as a man among\\nthe March Hare had thus fall of men, and you will find the same than\\nanso myself, but the men so thinking the wrote of the treachiness of the\\nmirth, and knew how at this corner Ulysses beggars and a dish outhime again. I\\nwonder if I shall five well to itself, hide all the world to my mother and exhibition,\\xe2\\x80\\x9d he\\nsaid to him the sun beathed the thors which sat that walk in the\\nfines box uncoming the pupils back to his feelings, in fact, as ever a Gryp\\nout line that I had him something in to face. This fellow may\\ncarry out with his feet, and wishes to ask to get presents to if you will.\\nI shall only hope the gods no more than I call after him\\xe2\\x80\\x94then, I ain\\xe2\\x80\\x99t\\ngoing to begin any?\\xe2\\x80\\x9d in its the same pea.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cOh, Tom, what\\xe2\\x80\\x99s the March Hare? if you\\xe2\\x80\\x99ll let Joe Harper, no, not even after'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London.\\n\\n\\xe2\\x80\\x9cSo ba quait the widow\\xe2\\x80\\x99s going to pulled it.\\xe2\\x80\\x9d He\\nsaid:\\n\\n\\xe2\\x80\\x9cGut your heer that the white Racedaemon, where he is gently away.\\n\\nI took dragged me truly and law scouts to me.\\xe2\\x80\\x99\\n\\n\\xe2\\x80\\x9cWhat\\xe2\\x80\\x99s the matter with you? She\\xe2\\x80\\x99ll either big bounce and tell he\\nwent to the dead. But it isn\\xe2\\x80\\x99t about that, that\\xe2\\x80\\x94lets wish,\\n    Be just they want to miss another, too,\\xe2\\x80\\x9d said\\nthat, it is full of beautiful lines 62, except that\\xe2\\x80\\x94never slipped along, so he went\\nout on the brick direction.\\n\\n\\xe2\\x80\\x9cWho is this no?\\xe2\\x80\\x9d Mr. Alcrect of Jove answered, \\xe2\\x80\\x9cMy\\npossession was well himself to stand with interest piece and fork of\\nthe yall. Then there was a large crafty of delicate that he sit at interesting that\\nthere was laid upon the mill and talled many Artonou and Dollus with an\\nimmoralt, and he would have stuck, and vinishered safely to his own, as usual.\\nThey weren\\xe2\\x80\\x99t anicable, so that with my miss\\nfine feeling, and show find you in same year; but nothing coming\\nhome to Ithaca; he has fond one sprang to it and sailed at me, and\\ntheir '\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London.\\n\\n\\xe2\\x80\\x9cYou will not creator Salum.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cNo, then yesterday more. He went straight through strong broad hands in\\nLemoph\\xe2\\x80\\x94rides----------------------------------------------------------------------------------------\\n\\nBeautiful, the sport can hardly do, where he wouldn\\xe2\\x80\\x99t stand there. They see for first on foot,\\nbut no finally it would not be too close to my own father. I will not\\nbe, for they thought I\\xe2\\x80\\x99d got to come on a nice blood-ewed, and that\\xe2\\x80\\x99s\\nthe talk around till at least any one with my crew, for Minerva had only\\nlet me leave it from home, nor leave him, you threw them on their tax setting\\nsupper it in the fire, and the Lory weavenking of mine caught it just as he was\\nthere that the great rock may bloom out impatiently, and stood loads in it at least\\n    so now come, too, that kept, own\\xe2\\x80\\x94and all the\\nheat of the town would be in a huff beyond the man who had given upon\\neach father\\xe2\\x80\\x99s name. They struggled to stury as though we had done breeting\\nthat he was a half, and shut as much as expectat'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London.\\n\\nWe went out and opened it; therefore it was getting more time as straight up to\\nDaisy and a taxing preyerable effort, to use on after the old melance of\\nthree boys were buring unserved by somebody from his mouth put his clay\\nhelp of distaff dropping upon the house in a thicket, he entered it\\nfrom its place in the air, and the towel and shoveled him to shinkle and\\nwere received with his neck. He cut the captive or a belt. Then she dealthing\\nfauntly impatiently: \\xe2\\x80\\x9cthe men may protect you.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cHonor, you need not stay here: they is gone! Good time for such a place that\\nIp thousands of people oftend no enchantils of contents; nevertheless, since they knew\\nGatsby\\xe2\\x80\\x99s father.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cHow do you like the Mock Turtle \\xe2\\x80\\x99em it had long since to me, let Daisy\\nto-knees he could draw, a swim out for Daisy poem feasting that I have travelled\\nmuch troubles enom most instance, and some other master or\\noaked the wind toss. I give it up Inother.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cWhy, whose hauseker\\xe2\\x80\\x9d I shall never, neither could bears in that'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.503626346588135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScGLb6XOgy-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24838564-9b4e-4d5e-bba4-06116342693f"
      },
      "source": [
        "tf.saved_model.save(mod, 'text-gen')\n",
        "text_gen = tf.saved_model.load('text-gen')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.step object at 0x7fedaedb8dd0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: text-gen/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: text-gen/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A2yaQRtgy6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ef12a7-d7a5-4aa8-9f5d-33c78cc640d0"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = text_gen.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London.\n",
            "\n",
            "I will fix does the wholesh, old friend or men, as it is, we pleased sulder, you not bring\n",
            "him to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFcOEsS_ku1j"
      },
      "source": [
        "class Training(next_step_model):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkokpFk0kuzB"
      },
      "source": [
        "model = Training(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp3MWIdBkuwS"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFLcSO3kuuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21896d96-257a-45b2-be00-4839992f7754"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232/232 [==============================] - 46s 182ms/step - loss: 2.5420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fed8587d210>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQC5aHUMkurq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7039272a-9ccf-43b3-aa1c-4ab969c889bb"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0676\n",
            "Epoch 1 Batch 50 Loss 1.9913\n",
            "Epoch 1 Batch 100 Loss 1.8678\n",
            "Epoch 1 Batch 150 Loss 1.7819\n",
            "Epoch 1 Batch 200 Loss 1.7545\n",
            "\n",
            "Epoch 1 Loss: 1.8743\n",
            "Time taken for 1 epoch 43.72 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.6931\n",
            "Epoch 2 Batch 50 Loss 1.6132\n",
            "Epoch 2 Batch 100 Loss 1.5528\n",
            "Epoch 2 Batch 150 Loss 1.5532\n",
            "Epoch 2 Batch 200 Loss 1.5082\n",
            "\n",
            "Epoch 2 Loss: 1.5966\n",
            "Time taken for 1 epoch 42.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.4808\n",
            "Epoch 3 Batch 50 Loss 1.4945\n",
            "Epoch 3 Batch 100 Loss 1.4096\n",
            "Epoch 3 Batch 150 Loss 1.4527\n",
            "Epoch 3 Batch 200 Loss 1.4294\n",
            "\n",
            "Epoch 3 Loss: 1.4511\n",
            "Time taken for 1 epoch 42.90 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.3468\n",
            "Epoch 4 Batch 50 Loss 1.3510\n",
            "Epoch 4 Batch 100 Loss 1.3914\n",
            "Epoch 4 Batch 150 Loss 1.3708\n",
            "Epoch 4 Batch 200 Loss 1.3473\n",
            "\n",
            "Epoch 4 Loss: 1.3636\n",
            "Time taken for 1 epoch 42.84 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.2995\n",
            "Epoch 5 Batch 50 Loss 1.3250\n",
            "Epoch 5 Batch 100 Loss 1.3072\n",
            "Epoch 5 Batch 150 Loss 1.3235\n",
            "Epoch 5 Batch 200 Loss 1.3007\n",
            "\n",
            "Epoch 5 Loss: 1.2959\n",
            "Time taken for 1 epoch 42.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2508\n",
            "Epoch 6 Batch 50 Loss 1.2682\n",
            "Epoch 6 Batch 100 Loss 1.2546\n",
            "Epoch 6 Batch 150 Loss 1.2515\n",
            "Epoch 6 Batch 200 Loss 1.2536\n",
            "\n",
            "Epoch 6 Loss: 1.2435\n",
            "Time taken for 1 epoch 42.83 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.1732\n",
            "Epoch 7 Batch 50 Loss 1.2123\n",
            "Epoch 7 Batch 100 Loss 1.2059\n",
            "Epoch 7 Batch 150 Loss 1.1713\n",
            "Epoch 7 Batch 200 Loss 1.1644\n",
            "\n",
            "Epoch 7 Loss: 1.1946\n",
            "Time taken for 1 epoch 42.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.0784\n",
            "Epoch 8 Batch 50 Loss 1.1248\n",
            "Epoch 8 Batch 100 Loss 1.1480\n",
            "Epoch 8 Batch 150 Loss 1.1380\n",
            "Epoch 8 Batch 200 Loss 1.1645\n",
            "\n",
            "Epoch 8 Loss: 1.1507\n",
            "Time taken for 1 epoch 42.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.0729\n",
            "Epoch 9 Batch 50 Loss 1.0970\n",
            "Epoch 9 Batch 100 Loss 1.1177\n",
            "Epoch 9 Batch 150 Loss 1.1252\n",
            "Epoch 9 Batch 200 Loss 1.1014\n",
            "\n",
            "Epoch 9 Loss: 1.1068\n",
            "Time taken for 1 epoch 42.96 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.0334\n",
            "Epoch 10 Batch 50 Loss 1.0427\n",
            "Epoch 10 Batch 100 Loss 1.0634\n",
            "Epoch 10 Batch 150 Loss 1.0882\n",
            "Epoch 10 Batch 200 Loss 1.0929\n",
            "\n",
            "Epoch 10 Loss: 1.0625\n",
            "Time taken for 1 epoch 42.95 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 11 Batch 0 Loss 0.9996\n",
            "Epoch 11 Batch 50 Loss 0.9841\n",
            "Epoch 11 Batch 100 Loss 1.0140\n",
            "Epoch 11 Batch 150 Loss 1.0496\n",
            "Epoch 11 Batch 200 Loss 1.0288\n",
            "\n",
            "Epoch 11 Loss: 1.0185\n",
            "Time taken for 1 epoch 42.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 12 Batch 0 Loss 0.9593\n",
            "Epoch 12 Batch 50 Loss 0.9233\n",
            "Epoch 12 Batch 100 Loss 0.9728\n",
            "Epoch 12 Batch 150 Loss 0.9730\n",
            "Epoch 12 Batch 200 Loss 0.9727\n",
            "\n",
            "Epoch 12 Loss: 0.9734\n",
            "Time taken for 1 epoch 42.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 13 Batch 0 Loss 0.8796\n",
            "Epoch 13 Batch 50 Loss 0.8983\n",
            "Epoch 13 Batch 100 Loss 0.9210\n",
            "Epoch 13 Batch 150 Loss 0.9522\n",
            "Epoch 13 Batch 200 Loss 0.9472\n",
            "\n",
            "Epoch 13 Loss: 0.9282\n",
            "Time taken for 1 epoch 42.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 14 Batch 0 Loss 0.8696\n",
            "Epoch 14 Batch 50 Loss 0.8498\n",
            "Epoch 14 Batch 100 Loss 0.8759\n",
            "Epoch 14 Batch 150 Loss 0.8830\n",
            "Epoch 14 Batch 200 Loss 0.9073\n",
            "\n",
            "Epoch 14 Loss: 0.8849\n",
            "Time taken for 1 epoch 42.75 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 15 Batch 0 Loss 0.7998\n",
            "Epoch 15 Batch 50 Loss 0.8168\n",
            "Epoch 15 Batch 100 Loss 0.8186\n",
            "Epoch 15 Batch 150 Loss 0.8490\n",
            "Epoch 15 Batch 200 Loss 0.8868\n",
            "\n",
            "Epoch 15 Loss: 0.8432\n",
            "Time taken for 1 epoch 42.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 16 Batch 0 Loss 0.7437\n",
            "Epoch 16 Batch 50 Loss 0.7596\n",
            "Epoch 16 Batch 100 Loss 0.8001\n",
            "Epoch 16 Batch 150 Loss 0.8364\n",
            "Epoch 16 Batch 200 Loss 0.8505\n",
            "\n",
            "Epoch 16 Loss: 0.8041\n",
            "Time taken for 1 epoch 42.75 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 17 Batch 0 Loss 0.7503\n",
            "Epoch 17 Batch 50 Loss 0.7190\n",
            "Epoch 17 Batch 100 Loss 0.7772\n",
            "Epoch 17 Batch 150 Loss 0.8177\n",
            "Epoch 17 Batch 200 Loss 0.7888\n",
            "\n",
            "Epoch 17 Loss: 0.7678\n",
            "Time taken for 1 epoch 42.84 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 18 Batch 0 Loss 0.7016\n",
            "Epoch 18 Batch 50 Loss 0.6745\n",
            "Epoch 18 Batch 100 Loss 0.7351\n",
            "Epoch 18 Batch 150 Loss 0.7626\n",
            "Epoch 18 Batch 200 Loss 0.7872\n",
            "\n",
            "Epoch 18 Loss: 0.7365\n",
            "Time taken for 1 epoch 42.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 19 Batch 0 Loss 0.6758\n",
            "Epoch 19 Batch 50 Loss 0.6658\n",
            "Epoch 19 Batch 100 Loss 0.7061\n",
            "Epoch 19 Batch 150 Loss 0.7119\n",
            "Epoch 19 Batch 200 Loss 0.7480\n",
            "\n",
            "Epoch 19 Loss: 0.7117\n",
            "Time taken for 1 epoch 42.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 20 Batch 0 Loss 0.6437\n",
            "Epoch 20 Batch 50 Loss 0.6489\n",
            "Epoch 20 Batch 100 Loss 0.6798\n",
            "Epoch 20 Batch 150 Loss 0.7055\n",
            "Epoch 20 Batch 200 Loss 0.7258\n",
            "\n",
            "Epoch 20 Loss: 0.6877\n",
            "Time taken for 1 epoch 42.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 21 Batch 0 Loss 0.6294\n",
            "Epoch 21 Batch 50 Loss 0.6383\n",
            "Epoch 21 Batch 100 Loss 0.6613\n",
            "Epoch 21 Batch 150 Loss 0.6851\n",
            "Epoch 21 Batch 200 Loss 0.7023\n",
            "\n",
            "Epoch 21 Loss: 0.6691\n",
            "Time taken for 1 epoch 42.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 22 Batch 0 Loss 0.6347\n",
            "Epoch 22 Batch 50 Loss 0.6282\n",
            "Epoch 22 Batch 100 Loss 0.6324\n",
            "Epoch 22 Batch 150 Loss 0.6512\n",
            "Epoch 22 Batch 200 Loss 0.6893\n",
            "\n",
            "Epoch 22 Loss: 0.6517\n",
            "Time taken for 1 epoch 42.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 23 Batch 0 Loss 0.6111\n",
            "Epoch 23 Batch 50 Loss 0.5998\n",
            "Epoch 23 Batch 100 Loss 0.6267\n",
            "Epoch 23 Batch 150 Loss 0.6551\n",
            "Epoch 23 Batch 200 Loss 0.6750\n",
            "\n",
            "Epoch 23 Loss: 0.6387\n",
            "Time taken for 1 epoch 42.76 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 24 Batch 0 Loss 0.6064\n",
            "Epoch 24 Batch 50 Loss 0.5829\n",
            "Epoch 24 Batch 100 Loss 0.6193\n",
            "Epoch 24 Batch 150 Loss 0.6429\n",
            "Epoch 24 Batch 200 Loss 0.6754\n",
            "\n",
            "Epoch 24 Loss: 0.6293\n",
            "Time taken for 1 epoch 42.74 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 25 Batch 0 Loss 0.5930\n",
            "Epoch 25 Batch 50 Loss 0.5858\n",
            "Epoch 25 Batch 100 Loss 0.6068\n",
            "Epoch 25 Batch 150 Loss 0.6655\n",
            "Epoch 25 Batch 200 Loss 0.6470\n",
            "\n",
            "Epoch 25 Loss: 0.6203\n",
            "Time taken for 1 epoch 42.74 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 26 Batch 0 Loss 0.5424\n",
            "Epoch 26 Batch 50 Loss 0.5648\n",
            "Epoch 26 Batch 100 Loss 0.6058\n",
            "Epoch 26 Batch 150 Loss 0.6244\n",
            "Epoch 26 Batch 200 Loss 0.6435\n",
            "\n",
            "Epoch 26 Loss: 0.6090\n",
            "Time taken for 1 epoch 42.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 27 Batch 0 Loss 0.5751\n",
            "Epoch 27 Batch 50 Loss 0.5633\n",
            "Epoch 27 Batch 100 Loss 0.5997\n",
            "Epoch 27 Batch 150 Loss 0.6151\n",
            "Epoch 27 Batch 200 Loss 0.6378\n",
            "\n",
            "Epoch 27 Loss: 0.6063\n",
            "Time taken for 1 epoch 42.65 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 28 Batch 0 Loss 0.5627\n",
            "Epoch 28 Batch 50 Loss 0.5660\n",
            "Epoch 28 Batch 100 Loss 0.5827\n",
            "Epoch 28 Batch 150 Loss 0.5984\n",
            "Epoch 28 Batch 200 Loss 0.6342\n",
            "\n",
            "Epoch 28 Loss: 0.5997\n",
            "Time taken for 1 epoch 42.62 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 29 Batch 0 Loss 0.5783\n",
            "Epoch 29 Batch 50 Loss 0.5620\n",
            "Epoch 29 Batch 100 Loss 0.5649\n",
            "Epoch 29 Batch 150 Loss 0.6152\n",
            "Epoch 29 Batch 200 Loss 0.6420\n",
            "\n",
            "Epoch 29 Loss: 0.5956\n",
            "Time taken for 1 epoch 42.57 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 30 Batch 0 Loss 0.5716\n",
            "Epoch 30 Batch 50 Loss 0.5377\n",
            "Epoch 30 Batch 100 Loss 0.5822\n",
            "Epoch 30 Batch 150 Loss 0.6286\n",
            "Epoch 30 Batch 200 Loss 0.6495\n",
            "\n",
            "Epoch 30 Loss: 0.5948\n",
            "Time taken for 1 epoch 42.74 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg6gqMRRkum7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918775f2-b8ab-4132-ad43-165d7aa218b7"
      },
      "source": [
        "#Prev model used several books combined\n",
        "#next model will test with several books by the same author\n",
        "path_to_finn = tf.keras.utils.get_file('76.txt.txt', 'https://www.gutenberg.org/files/76/76-0.txt')\n",
        "path_to_prince = tf.keras.utils.get_file('1837-0.txt', 'https://www.gutenberg.org/files/1837/1837-0.txt')\n",
        "\n",
        "finn = open(path_to_alice, 'rb').read().decode(encoding='utf-8')\n",
        "prince = open(path_to_gatsby, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "text = tom + finn + prince\n",
        "\n",
        "print('Length of mark twain text: {} characters'.format(len(text)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/76/76-0.txt\n",
            "614400/610370 [==============================] - 0s 0us/step\n",
            "622592/610370 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/1837/1837-0.txt\n",
            "434176/431701 [==============================] - 0s 0us/step\n",
            "442368/431701 [==============================] - 0s 0us/step\n",
            "Length of mark twain text: 826335 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpGhZHPzkufX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5466174b-275b-4510-f57f-b2f0bebeb6b8"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96 unique characters\n",
            "['\\t', '\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ç', 'é', 'ê', 'ô', 'ù', '\\u200a', '—', '‘', '’', '“', '”', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v_dCLAwYqP"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmSFWMhdwYn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f70d09-e460-4118-aa99-98a0f49f19da"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")\n",
        "  \n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(826335,), dtype=int64, numpy=array([ 2, 49, 37, ...,  2,  2,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPB94rabwYmD"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_HeBIfLwYki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ba4416-73ed-43a0-bd59-09651c931a60"
      },
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLPTIPOawYhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750644fa-6a27-4c00-aeab-c50a351d7d90"
      },
      "source": [
        "model = next_step_model(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "    \n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 97) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"next_step_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  24832     \n",
            "                                                                 \n",
            " gru_3 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  99425     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,062,561\n",
            "Trainable params: 4,062,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBSpOVyVwYem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3c569f-b17b-4fcb-db19-9a912aac16fd"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([49, 78, 88,  5, 41,  9, 47, 76, 59,  1, 77, 79, 20, 32, 87, 51, 33,\n",
              "       94, 80, 61, 89,  2, 71, 33, 33, 28, 20,  8, 26, 13, 36, 60, 50, 85,\n",
              "       36, 14, 85, 53,  4, 58, 66, 86, 87, 49, 23, 82, 57, 30, 93, 95, 90,\n",
              "       79, 23, 67, 59, 22, 57, 31, 70, 57,  6,  6, 63,  9, 70, 12, 40, 38,\n",
              "       41, 12,  3, 37,  6, 80, 20, 67, 78, 50, 93, 19, 63, 21, 80, 58, 28,\n",
              "       28, 10, 64, 85, 29, 81, 30, 36, 92, 21, 18, 82, 17, 88, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yNxzhhhwYcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3e3d46-8377-4978-a808-0ec583deb8c8"
      },
      "source": [
        "print(\"Input:\\n\", chars_from_ids(input_example_batch[0]))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", chars_from_ids(sampled_indices))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tf.Tensor(\n",
            "[b'h' b'e' b' ' b's' b'a' b't' b' ' b'w' b'i' b't' b'h' b' ' b'D' b'a'\n",
            " b'i' b's' b'y' b' ' b'i' b'n' b' ' b'h' b'i' b's' b'\\n' b'a' b'r' b'm'\n",
            " b's' b' ' b'f' b'o' b'r' b' ' b'a' b' ' b'l' b'o' b'n' b'g' b',' b' '\n",
            " b's' b'i' b'l' b'e' b'n' b't' b' ' b't' b'i' b'm' b'e' b'.' b' ' b'I'\n",
            " b't' b' ' b'w' b'a' b's' b' ' b'a' b' ' b'c' b'o' b'l' b'd' b' ' b'f'\n",
            " b'a' b'l' b'l' b' ' b'd' b'a' b'y' b',' b' ' b'w' b'i' b't' b'h' b' '\n",
            " b'f' b'i' b'r' b'e' b' ' b'i' b'n' b' ' b't' b'h' b'e' b'\\n' b'r' b'o'\n",
            " b'o' b'm'], shape=(100,), dtype=string)\n",
            "\n",
            "Next Char Predictions:\n",
            " tf.Tensor(\n",
            "[b'T' b't' b'\\xc3\\xb4' b'\"' b'L' b\"'\" b'R' b'r' b'a' b'\\t' b's' b'u' b'3'\n",
            " b'C' b'\\xc3\\xaa' b'V' b'D' b'\\xe2\\x80\\x9c' b'v' b'c' b'\\xc3\\xb9' b'\\n'\n",
            " b'm' b'D' b'D' b';' b'3' b'&' b'9' b',' b'G' b'b' b'U' b'\\xc3\\xa7' b'G'\n",
            " b'-' b'\\xc3\\xa7' b'X' b'!' b'_' b'h' b'\\xc3\\xa9' b'\\xc3\\xaa' b'T' b'6'\n",
            " b'x' b']' b'A' b'\\xe2\\x80\\x99' b'\\xe2\\x80\\x9d' b'\\xe2\\x80\\x8a' b'u' b'6'\n",
            " b'i' b'a' b'5' b']' b'B' b'l' b']' b'$' b'$' b'e' b\"'\" b'l' b'*' b'K'\n",
            " b'I' b'L' b'*' b' ' b'H' b'$' b'v' b'3' b'i' b't' b'U' b'\\xe2\\x80\\x99'\n",
            " b'2' b'e' b'4' b'v' b'_' b';' b';' b'(' b'f' b'\\xc3\\xa7' b'?' b'w' b'A'\n",
            " b'G' b'\\xe2\\x80\\x98' b'4' b'1' b'x' b'0' b'\\xc3\\xb4' b'9'], shape=(100,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRZFNqWcwYaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8baa29fd-9c6f-4c99-dc53-fee30471ac24"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 97)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.5757165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNTQeCpswqPb"
      },
      "source": [
        "tf.exp(mean_loss).numpy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZsSqSG6wqNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7425dd12-51c2-47e4-9b7f-9fc475ce3c24"
      },
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "127/127 [==============================] - 26s 180ms/step - loss: 2.8829\n",
            "Epoch 2/30\n",
            "127/127 [==============================] - 24s 181ms/step - loss: 2.1847\n",
            "Epoch 3/30\n",
            "127/127 [==============================] - 24s 181ms/step - loss: 1.9079\n",
            "Epoch 4/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.7085\n",
            "Epoch 5/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.5685\n",
            "Epoch 6/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.4670\n",
            "Epoch 7/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.3888\n",
            "Epoch 8/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.3269\n",
            "Epoch 9/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.2713\n",
            "Epoch 10/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 1.2215\n",
            "Epoch 11/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 1.1724\n",
            "Epoch 12/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 1.1225\n",
            "Epoch 13/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 1.0718\n",
            "Epoch 14/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 1.0207\n",
            "Epoch 15/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.9637\n",
            "Epoch 16/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.9073\n",
            "Epoch 17/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.8481\n",
            "Epoch 18/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.7874\n",
            "Epoch 19/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.7288\n",
            "Epoch 20/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.6704\n",
            "Epoch 21/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.6168\n",
            "Epoch 22/30\n",
            "127/127 [==============================] - 24s 180ms/step - loss: 0.5666\n",
            "Epoch 23/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.5232\n",
            "Epoch 24/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.4838\n",
            "Epoch 25/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.4507\n",
            "Epoch 26/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.4215\n",
            "Epoch 27/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.3969\n",
            "Epoch 28/30\n",
            "127/127 [==============================] - 24s 178ms/step - loss: 0.3780\n",
            "Epoch 29/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.3626\n",
            "Epoch 30/30\n",
            "127/127 [==============================] - 24s 179ms/step - loss: 0.3481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1x-1eQqwqK-"
      },
      "source": [
        "mod = step(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaARzFtvwqIh"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHsXLrtAwqGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0601ac-0737-4a94-b147-f9c298776160"
      },
      "source": [
        "for n in range(1000):\n",
        "  next_char, states = mod.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London. The\n",
            "widow Douglas, the dancing breakfastored and gave out a bsyoutial for hadge of Alite,\n",
            "and the two or three times (when I had if they can’t feel badly and corner of his anike, and ware about\n",
            "the right time how lay in the world. It was\n",
            "delighted to some evening behind. To Souther opened into a doze\n",
            " Silence, In an unfroluction that the boys thought at the ticking of\n",
            "a declaim, “Long Island oh! that toe don’t I dog and New Losk left, and a-while\n",
            "an old book for I knew now. I want to go and death, and then I live it?”\n",
            "\n",
            "“I bleeve it’s done and don’t forget all the thing.”\n",
            "\n",
            "I was bound to give up the year and there was a wholesome burst of the\n",
            "wife, be-aroudged and cleared the policeman, the walletor of the Lord’s was a thing before\n",
            "whose fause ir gathered his fist in his bed at night.\n",
            "\n",
            "A gor head was played tonessup into the effort. And before I listened at last, and\n",
            "tearing his weakness, a quarter of a mile, so vanited to fish, but the\n",
            "sense of the birds posted on a little bleed-sky a \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 6.210324764251709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6JxgCXCwqCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfbaaa6-2c79-4c75-9c0a-05c6ce08bcf7"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.', 'The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = mod.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'The world seemed like such a peaceful place until the magic tree was discovered in London. The bore\\nwere no langer and slipped in the room when it was quite impulse.\\n\\nTom broke into the air, mixed up, with their eyes to live behind. There\\nwas a pin about his face could bedrove. They began to lay their love at her.\\n\\n\\xe2\\x80\\x9cLet us delibved to dig next, then?\\xe2\\x80\\x9d he added \\xe2\\x80\\x9cas Muff\\xe2\\x80\\x99s Plung,\\nwho cares, Alice looked at it, and one boy in the towel, he had to sing\\n\\nThe Graviar Harper and George Big Mave\\nmelancholy. She got up to take a very feeble and among them.\\n\\nAlice waited captured and not a yathout interish kept into the\\nroom. He looked at them\\xe2\\x80\\x94\\xe2\\x80\\x9cI wash a\\nsensation, the tyrninons. An I, the rest of June, blow violently before he\\nwent on, \\xe2\\x80\\x9cand of boys in that pine, but a week\\nthat many longer real things\\xe2\\x80\\x9d she said sofeten, ammirl in\\ntrush, in the place with the moral and bursts and among gold and said the light.\\n\\nAll right it was more of a succuse. Doesn\\xe2\\x80\\x99t he?\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cBut what worry,\\xe2\\x80\\x9d he said, \\xe2\\x80\\x9cit was, now. What\\xe2\\x80\\x99s yours too?\\xe2\\x80\\x9d\\n The moon had made a pieces of blue boat, loads on their agged eye'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London. For a little belitted to eat, and\\nfearing his eye were going to me that the piece in confinsable stump within another was a season, and moved\\nirresolutely about, in writing, and the baby was his idable. Sonething was inviladed.\\n\\n\\xe2\\x80\\x9cIt\\xe2\\x80\\x99s only half-taily Joe Harper about my neighbour\\xe2\\x80\\x99s, or its dun,\\xe2\\x80\\x9d the Hatter came. \\xe2\\x80\\x9cAt first\\nI bet you that \\xe2\\x80\\x98\\xe2\\x80\\x99mostor you.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cYes, but then around Jordan\\xe2\\x80\\x99s gold that did and his\\npersonecion in him; and he looked at Alice he jumped to the back train.\\n\\nThe boys dead sort of things to be a terrible state, with a shake of\\nbeasts, and finally his hatched her slip in the room.\\n\\n\\xe2\\x80\\x9cI\\xe2\\x80\\x99m a\\xe2\\x80\\x94\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cI like here?\\xe2\\x80\\x9d And he added in a low, turning tie. When she\\nand the exclaimit of the village, and some from of up to wire a little back the situation of a\\ncompation was untructibel Evidence and ghosts as to speak at all.\\n\\nForms, till he found it and walked to be breathly, and began\\nto repeat it, but her hand began to distro Michaelis, took the other air.\\n\\n\\xe2\\x80\\x9cI\\xe2\\x80\\x99m very important.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cI '\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London. When I think it\\nwasn\\xe2\\x80\\x99t one. A tick was even when they had been\\nthat green said with joy. He said to himself that he would be quite abjuct as\\nlong as he could. Mutiny waited for them at him. Well, I\\xe2\\x80\\x99ll go answering. Now the more\\nred-and-ghosts of the works\\xe2\\x80\\x94pointing there on the situation\\xe2\\x80\\x94in her losings, washed anything; they\\nconveyed it over) thought it all over again, but then alone the\\nname a little way off, leaving them: she carried, unlasted. Every defiant juvenine-smokeneds and the\\nRabbit Herro, and Sid was soon read:\\n\\n\\xe2\\x80\\x9cTom, would you, Tom? You wouldn\\xe2\\x80\\x99t stand up. The idea is\\nin pencil, Is causing a row, making a white and laves me before _mostic very timped out alone, and the unflagration of a\\nsilence and the began to risk of light, took a rape to ease an irresistible journer, Tom was\\nline a home, he kept their lovest\\nsunfiting the sub-control.\\xe2\\x80\\x9d remarked Tok appointed, they\\nhad been walking for buint and gazed a bit, a presenties, and faint out\\nat the house and sking out of his kn'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London. I needn\\xe2\\x80\\x99t even\\nstood in the room way through the name of seat.\\n\\nThe interior of the edge we close being between torself about him in his aunt\\xe2\\x80\\x99s fellow,\\nas the two boinspripped upon all softed eggs, sevorated class,\\nbutting halt a dozen times came and went to the resp.\\n\\n\\xe2\\x80\\x9cIS it had traded me for you?\\xe2\\x80\\x9d And she sat up, \\xe2\\x80\\x9cand I do leave the way the\\no\\xe2\\x80\\x99chon-croached Wilcours, I donged soon that day, Tom. Tom! It\\xe2\\x80\\x99s on the blate bunday myself, but\\nWolfshiem trave this way! St. Petersburg and his grief ten shower on herself, and\\nshe was quickly useful, and looked at Alice.\\n\\n\\xe2\\x80\\x9cIt must be a very detal, by the piece if you were like thet?\\xe2\\x80\\x9d and\\nshe turned scrawled. \\xe2\\x80\\x9cI\\xe2\\x80\\x99ll pretty going to look after o\\xe2\\x80\\x99r all the way down Don\\nIndian, and a little bit of sticks and disorpersation. It\\nwas a white and lave with a fore after them after it; therefore\\nbefore, so they pleasure response to be a doclor himself. Tom Biloxion, Daisy,\\nwondering very melancholy avondshore.\\n\\n\\xe2\\x80\\x9cMrand time I was down the reason so, we\\xe2\\x80\\x99dl'\n",
            " b'The world seemed like such a peaceful place until the magic tree was discovered in London. The bent failt for crazy, and\\nneither of the other baks and thought to herself, \\xe2\\x80\\x9cWhy, they\\xe2\\x80\\x99re about!\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cDon\\xe2\\x80\\x99t yet, Mr. Gatsby! Everybody does. Least I hadn\\xe2\\x80\\x99t come?\\xe2\\x80\\x9d\\n\\nMrs. Wolance, his eyes, sick in his hopes upon an enchestrails of\\nfact, a pale well, rippling and flowered. The sunlights returned flash off his\\nmiddle of switches and the whole school to enjoy it. The boys were avenged. Vacation room that entered with laughter.\\n\\nThe usual white arms moved in my mind were the voice of the hedgehog\\nalways a few of them, and the stage struggled catch his side, and had the haunted\\nhouse and passed an indeterment, would be touch that he had been very\\ncautiously at the Gryphon. Tom struggled among the banjagorn liftite back, and at last something\\nto be a piece of raid hours he forgot\\nalthatching loneliness and full of time. It was almost smiled.\\n\\n\\xe2\\x80\\x9cI fliep of water, My! but we\\xe2\\x80\\x99ll come to it, that\\xe2\\x80\\x99s all you know. No\\xe2\\x80\\x94you\\xe2\\x80\\x99re arrived, and then\\nshe had an immediately deepy wonder. A large pig, made '], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.522757053375244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGzmlkApwp9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f82cd9-1d76-44ec-9b00-e70263e701e0"
      },
      "source": [
        "tf.saved_model.save(mod, 'twain-text-gen')\n",
        "twain_gen = tf.saved_model.load('twain-text-gen')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = twain_gen.generate_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.step object at 0x7fed84f14c90>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.step object at 0x7fed84f14c90>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: twain-text-gen/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: twain-text-gen/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London. For a little walls were\n",
            "somewhere are strange, when the battle was doing are about\n",
            "it: for the girl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v14Bkwb-wp7i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj2pCIfrwp44"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kqw1Ug3wp2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc-Yzr0wp0S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}